{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfc3af6-cd2c-473f-9380-46585e711e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a21cb98-8092-47b7-bcb4-d023ee3dbd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"C:\\\\Users\\\\DELL\\\\Downloads\\\\archive\\\\Training\"\n",
    "test_dir = \"C:\\\\Users\\\\DELL\\\\Downloads\\\\archive\\\\Testing\"\n",
    "\n",
    "# Augmentation and rescaling\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbad933-e6a8-4f7f-9f81-7f711f53c380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 1/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14708s\u001b[0m 82s/step - accuracy: 0.4793 - loss: 1.1194 - val_accuracy: 0.3402 - val_loss: 3.2077 - learning_rate: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 2/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 5s/step - accuracy: 0.6182 - loss: 0.9106 - val_accuracy: 0.5072 - val_loss: 1.4515 - learning_rate: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 3/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m854s\u001b[0m 5s/step - accuracy: 0.6330 - loss: 0.8608 - val_accuracy: 0.4905 - val_loss: 1.9067 - learning_rate: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 4/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1356s\u001b[0m 8s/step - accuracy: 0.6539 - loss: 0.8259 - val_accuracy: 0.5400 - val_loss: 1.5076 - learning_rate: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 5/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m891s\u001b[0m 5s/step - accuracy: 0.6689 - loss: 0.7991 - val_accuracy: 0.6941 - val_loss: 0.7772 - learning_rate: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 6/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m860s\u001b[0m 5s/step - accuracy: 0.6956 - loss: 0.7413 - val_accuracy: 0.6720 - val_loss: 0.8664 - learning_rate: 1.0000e-04\n",
      "Learning rate:  1e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m846s\u001b[0m 5s/step - accuracy: 0.7211 - loss: 0.6819 - val_accuracy: 0.6735 - val_loss: 0.8080 - learning_rate: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1122s\u001b[0m 6s/step - accuracy: 0.7318 - loss: 0.6620 - val_accuracy: 0.6804 - val_loss: 0.7796 - learning_rate: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m878s\u001b[0m 5s/step - accuracy: 0.7297 - loss: 0.6489 - val_accuracy: 0.6766 - val_loss: 0.7943 - learning_rate: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 5s/step - accuracy: 0.7635 - loss: 0.6082 - val_accuracy: 0.7048 - val_loss: 0.7090 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = True\n",
    "# Unfreeze the last 50 layers of ResNet50\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-4\n",
    "    if epoch > 5:\n",
    "        lr = 1e-5\n",
    "    elif epoch > 10:\n",
    "        lr = 1e-6\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train and test generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"brain_tumor_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99e3c5-4c34-4bd2-8975-5776f23176be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3adb43-e555-4896-b77d-aad06922bdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e257a2c2-2e6d-4515-8fd1-8ab2fa47167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 5s/step - accuracy: 0.6924 - loss: 0.7568 - val_accuracy: 0.5523 - val_loss: 1.7465 - learning_rate: 1.0000e-06\n",
      "Epoch 12/15\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 5s/step - accuracy: 0.7838 - loss: 0.5456 - val_accuracy: 0.6941 - val_loss: 0.7265 - learning_rate: 1.0000e-06\n",
      "Epoch 13/15\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 5s/step - accuracy: 0.8027 - loss: 0.4982 - val_accuracy: 0.7689 - val_loss: 0.5448 - learning_rate: 1.0000e-06\n",
      "Epoch 14/15\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m909s\u001b[0m 5s/step - accuracy: 0.8202 - loss: 0.4685 - val_accuracy: 0.7826 - val_loss: 0.5083 - learning_rate: 1.0000e-06\n",
      "Epoch 15/15\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m920s\u001b[0m 5s/step - accuracy: 0.8388 - loss: 0.4286 - val_accuracy: 0.7979 - val_loss: 0.4816 - learning_rate: 1.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import os\n",
    "\n",
    "# Load your model from epoch 10\n",
    "model = load_model(\"brain_tumor_model.h5\")\n",
    "\n",
    "# Unfreeze more layers for fine-tuning (e.g., last 70 layers)\n",
    "for layer in model.layers[-70:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler: tiny bump down mid-fine-tuning\n",
    "def lr_schedule(epoch):\n",
    "    return 1e-5 if epoch < 2 else 1e-6\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Data generators\n",
    "train_dir = \"C:\\\\Users\\\\DELL\\\\Downloads\\\\archive\\\\Training\"\n",
    "test_dir = \"C:\\\\Users\\\\DELL\\\\Downloads\\\\archive\\\\Testing\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224),\n",
    "                                                    batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(224, 224),\n",
    "                                                  batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Continue training (epochs 11–15)\n",
    "model.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          epochs=15,\n",
    "          initial_epoch=10,\n",
    "          callbacks=[lr_scheduler, early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# Save updated model\n",
    "model.save(\"brain_tumor_model_finetuned_epoch15.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddee38-ef21-44cc-8b55-7a51bacb21a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd40bbe-0e37-46b4-83b2-2a7ca4d6e92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3909f13-eb72-4690-ae82-83d6d53c19f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca99dd6-9d2c-4855-892c-0062e17a505a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713e77ae-ef3f-44f4-807d-95258b016810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef4a17f-0a37-48cf-8f87-0fba0f031043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DELL'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86320308-1d8f-42f0-a2e6-2321ecc7015b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21fdfc-b230-49b8-9b03-6fbff7e8b220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc6785-53fa-4073-a98a-35d640dd8610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a68924-7232-46f0-8a93-7c580573588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the image for prediction:  C:\\Users\\DELL\\Downloads\\archive\\Testing\\meningioma\\Te-me_0013.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001E6D4A17E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001E6D4A17E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Raw probabilities: [[0.00411899 0.2562462  0.5740081  0.16562678]]\n",
      "Predicted class: No_Tumor\n",
      "Confidence: 57.40%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Top 3 predictions:\n",
      "No_Tumor: 57.40%\n",
      "Meningioma: 25.62%\n",
      "Pituitary: 16.56%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load your model (after training or fine-tuning)\n",
    "model = load_model(\"brain_tumor_model_finetuned_epoch15.h5\")\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['Glioma', 'Meningioma', 'No_Tumor', 'Pituitary']\n",
    "\n",
    "# Function to preprocess the input image (rescale, resize, etc.)\n",
    "def preprocess_image(img_path):\n",
    "    # Load the image with the required target size for the model\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Normalize the image (same as you did during training)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Add an extra batch dimension (this is required for the model)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Function to predict the class of the input image\n",
    "def predict_tumor_type(img_path):\n",
    "    img_array = preprocess_image(img_path)\n",
    "    \n",
    "    # Make a prediction (get the probabilities for each class)\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # Show raw probabilities (useful for debugging)\n",
    "    print(\"Raw probabilities:\", prediction)\n",
    "    \n",
    "    # Get the class with the highest probability\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    \n",
    "    # Map it to the corresponding label\n",
    "    predicted_class = class_labels[predicted_class_index]\n",
    "    \n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    \n",
    "    # Return the class and confidence level\n",
    "    confidence = prediction[0][predicted_class_index] * 100\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Testing the function: Ask user for an image file path\n",
    "img_path = input(\"Enter the path to the image for prediction: \")\n",
    "\n",
    "# Get the prediction result\n",
    "predicted_class, confidence = predict_tumor_type(img_path)\n",
    "\n",
    "# Optionally, print the top-3 most likely classes with their probabilities\n",
    "def top_3_predictions(img_path):\n",
    "    img_array = preprocess_image(img_path)\n",
    "    \n",
    "    # Get probabilities for each class\n",
    "    probs = model.predict(img_array)[0]\n",
    "    \n",
    "    # Get the top 3 predicted classes\n",
    "    top_3 = np.argsort(probs)[-3:][::-1]\n",
    "    \n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i in top_3:\n",
    "        print(f\"{class_labels[i]}: {probs[i]*100:.2f}%\")\n",
    "    \n",
    "top_3_predictions(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37689bef-9cd0-48e1-ae2e-a2c68c30725e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e67a5-78e8-4480-8a94-5c8e43d53ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44815df9-1559-4b90-b22a-95cdf263fab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454eeaa8-0c2c-4cc3-8806-85d022dc9ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9d3f8-2b00-4fb9-a102-773a10080095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
